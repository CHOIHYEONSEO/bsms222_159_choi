---
title: "R Notebook"
output: html_notebook
---


# Chapter 15. Statistical inference

## 15.1 Polls
Polls are useful when interviewing every member of a particular population is logistically impossible. The general strategy is to interview a smaller group, chosen at random, and then infer the opinions of the entire population from the opinions of the smaller group.

_confidence intervals_
_p-values_
_Bayesian modeling_


### 15.1.1 Theh sampling model for polls
The dslabs package includes a function that shows a random draw from this urn:
```{r}
install.packages("tidyverse")
install.packages("dslabs")
library(tidyverse)
library(dslabs)
take_poll(25)
```



## 15.2 Populations, samples, parameters, and estimates
Let's call blue bead in the urn quantity **p**, which then tells us the proportion of red beads **1-p**, and the `spread` p-(1-p), which simplifies to **2p-1**.

In statistical textbooks, the beads in the urn are called the `population`. The proportion of blue beads in the population, `p` is called a `parameter`. The 25 beads we see in the previous plot are called a `sample`. The task of statistical inference is to predict the parameter p using the observed data in the sample.


### 15.2.1 The sample average
We start by defining the random variable X as:  
X = 1 if we pick a blue bead at random and X = 0 if it is red. This implies that the population is a list of 0s and 1s. 
If we sample N beads, then the average of the draws X1, …, XN is equivalent to the proportion of blue beads in our sample. This is because adding the Xs is equivalent to counting the blue beads and dividing this count by the total N is equivalent to computing a proportion. We use the symbol **Xbar** to represent this average. In general, in statistics textbooks a bar on top of a symbol means the average. The theory we just learned about the sum of draws becomes useful because the average is a sum of draws multiplied by the constant 1/N


### 15.2.2 Paramters
In the urn model which we are using to mimic an opinion poll, we do not know the proportion of blue beads in the urn. We define the _parameters_ p to represent this quantity. p is the average of the urn because if we take the average of the 1s (blue) and 0s (red), we get the proportion of blue beads. Since our main goal is figuring out what is p, we are going to _estimate this parameter_.



### 15.2.3 Polling versus forecasting
forecasters try to build tools that model how opinions vary across time and try to predict the election night results taking into consideration the fact that opinions fluctuate.
시간따라 의견 변함

#### 15.2.4 Properties of our estimate: expected value and standard error
Using what we have learned, the expected value of the sum NXbar is N × the average of the urn, p. So dividing by the non-random constant N gives us that the expected value of the average Xbar is p. We can write it using our mathematical notation:
_E(Xbar) = p_

We can also use what we learned to figure out the standard error: the standard error of the sum is 
√N × the standard deviation of the urn. Can we compute the standard error of the urn? We learned a formula that tells us that it is (1−0)√p(1−p) = √p(1−p). Because we are dividing the sum by N, we arrive at the following formula for the standard error of the average:
_SE(Xbar) = √p(1−p)/N_

This result reveals the power of polls. The expected value of the sample proportion Xbar is the parameter of interest p and we can make the standard error as small as we want by increasing N. The law of large numbers tells us that with a large enough poll, our estimate converges to p.

For a sample size of 1,000 and p=0.51, the standard error is:
```{r}
p = 0.51
sqrt(p*(1-p))/sqrt(1000)
```


## 15.3 Exercises
1. Suppose you poll a population in which a proportion p of voters are Democrats and 1 − p are Republicans. Your sample size is N = 25. Consider the random variable S which is the total number of Democrats in your sample. What is the expected value of this random variable? Hint: it’s a function of p.
```{r}
25*(1*p + 0*(1-p)) = 25p
```


2.  What is the standard error of S? Hint: it’s a function of p.
```{r}
5*sqrt(p*(1-p))
```


3. Consider the random variable S/N. This is equivalent to the sample average, which we have been denoting as Xbar. What is the expected value of the Xbar? Hint: it’s a function of p.
```{r}
p
```


4. What is the standard error of Xbar? Hint: it’s a function of p.
```{r}
sqrt(p*(1-p))/5
```


5. Write a line of code that gives you the standard error `se` for the problem above for several values of p, specifically for `p <- seq(0, 1, length = 100)`. Make a plot of `se` versus `p`.
```{r}
p <- seq(0, 1, length = 100)
se <- 5*sqrt(p*(1-p))
plot(p, se)
```

6.  Copy the code above and put it inside a for-loop to make the plot for N=25, N=100, and N=1000.
```{r}
my_function <- function(N){
  se <- sqrt(N)*sqrt(p*(1-p))
plot(p, se)
}

N <- c(25, 100, 1000)
sapply(N, my_function)
```

7. If we are interested in the difference in proportions, p-(1-p), our estimate is  d = Xbar - (1-Xbar). Use the rules we learned about sums of random variables and scaled random variables to derive the expected value of d.
```{r}
d = 2Xbar-1
E(d) = E(2Xbar-1) = 2*E(Xbar)-1 = 2*p-1
```

8. What is the standard error of d?
```{r}
SE(d) = SE(2Xbar-1) = 5*sd(2Xbar-1) = 10*sd(X) = 10*sqrt(p*(1-p))
```

9. If the actual p = 0.45, it means the Republicans are winning by a relatively large margin since d= -0.1, which is a 10% margin of victory. In this case, what is the standard error of 2Xbar - 1 if we take a sample of N=25?
```{r}
p <- 0.45
2*p - 1
```

10.  Given the answer to 9, which of the following best describes your strategy of using a sample size of N = 25?
```{r}
sqrt(p*(1-p)/25)
```
**The expected value of our estimate 2Xbar - 1 is d, so our prediction will be right on.**



## 15.4 Central Limit Theorem in practice
Pr(|Xbar - p| <= 0.1) = Pr(|Z| <= 0.1/Se(Xbar))

p를 몰라 SE(Xbar)를 모른다
: Xbar는 normal distribution과 거의 유사하므로 기댓값 p대신 Xbar대입
>Sehat(Xbar) = sqrt(Xbar(1-Xbar)/N)
In statistics textbooks, we use a little hat to denote estimates. The estimate can be constructed using the observed data and N

we had 12 blue and 13 red so Xbar=0.48 and our estimate of standard error is:
```{r}
x_hat <- 0.48
se <- sqrt(x_hat*(1-x_hat)/25)
se
```

nd now we can answer the question of the probability of being close to p. The answer is:
```{r}
pnorm(0.01/se) - pnorm(-0.01/se)
```
Therefore, there is a small chance that we will be close. A poll of only N = 25 people is not really very useful, at least not for a close election.

Earlier we mentioned the _margin of error_. Now we can define it because it is simply two times the standard error, which we can now estimate. In our case it is:
```{r}
1.96*se
```

Why do we multiply by 1.96? Because if you ask what is the probability that we are within 1.96 standard errors from p, we get:
Pr(Z <= 1.96SE(Xbar)/Se(Xbar)) - Pr(Z <= -1.96SE(Xbar)/Se(Xbar))
Pr(Z <= 1.96) - Pr(Z <= -1.96)
shich we know is about 95%.
```{r}
pnorm(1.96)-pnorm(-1.96)
```

Hence, there is a 95% probability that Xbar will be within 1.96*SEhab(Xbar), in our case within about 0.2, of p. Note that 95% is somewhat of an arbitrary choice and sometimes other percentages are used, but it is the most commonly used value to define margin of error. We often round 1.96 up to 2 for simplicity of presentation.

In summary, the CLT tells us that our poll based on a sample size of 25 is not very useful.
From the table above, we see that typical sample sizes range from 700 to 3500.


### 15.4.1 A Monte Carlo simulation
Suppose we want to use a Monte Carlo simulation to corroborate the tools we have built using probability theory. To create the simulation, we would write code like this:
```{r}
B <- 10000
N <- 1000
x_hat <- replicate(B, {
  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
})
```
The problem is, of course, we don’t know `p`. We could construct an urn like the one pictured above and run an analog (without a computer) simulation. It would take a long time, but you could take 10,000 samples, count the beads and keep track of the proportions of blue. We can use the function `take_poll(n=1000)` instead of drawing from an actual urn, but it would still take time to count the beads and enter the results.

One thing we therefore do to corroborate theoretical results is to pick one or several values of `p` and run the simulations. Let’s set `p=0.45`. We can then simulate a poll:

지금 p를 모르는 상태에서
1. 직접 환경을 설정하여서 계속 뽑아 p의 값 추측(take_poll())
2. 여러개의 임의의 p의 값을 집이넣어 추측
의 두가지 방법을 쓸 수 있다.

```{r}
p <- 0.45
N <- 1000

x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
x_hat <- mean(x)
```

In this particular sample, our estimate is x_hat. We can use that code to do a Monte Carlo simulation:
```{r}
B <- 10000
x_hat <- replicate(B, {
  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
})
```

To review, the theory tells us that Xbar is approximately normally distributed, has expected value p=0.45 and standard error √p(1−p)/N = 0.016. The simulation confirms this:
```{r}
mean(x_hat)
sd(x_hat)
```

A histogram and qq-plot confirm that the normal approximation is accurate as well:

Of course, in real life we would never be able to run such an experiment because we don’t know  
p. But we could run it for various values of p and N and see that the theory does indeed work well for most values. You can easily do this by re-running the code above after changing p and N.



### 15.4.2 The spread
spread : p- (1-p) = 2p-1
standard error : SE(2Xbar - 1) = 2SE(X)

### 15.4.3 Bias: why not run a very large poll?
 Historically, we observe that polls are indeed biased, although not by that much. The typical bias appears to be about 1-2%. This makes election forecasting a bit more interesting and we will talk about how to model this in a later chapter.
 
N이 아주 크면
1. 비쌈
2. 이론과 다르게 현실의 변수가 존재함 : 누가 urn안에 속해 있는가?


## 15.5 Exercises

1. Write an urn model function that takes the proportion of Democrats p and the sample size N as arguements and returns the sample average if Democrats are 1s and Republicans are 0s. Call the function `take_sample`.
```{r}
take_sample <- function(N){
  sample(c(0,1), N, replace = TRUE, prob = c(p, 1-p))
}
```

2. Now assume `p <- 0.45` and that your sample size is
N = 100. Take a sample 10,000 times and save the vector of `mean(X) - p` into an object called `errors`. Hint: use the function you wrote for exercise 1 to write this in one line of code.
```{r}
p <-0.45
N <- 100

errors <- replicate(10000, mean(take_sample(N))-p)
```

3. The vector errors contains, for each simulated sample, the difference between the actual p and our estimate Xbar. We refer to this difference as the error. Compute the average and make a histogram of the errors generated in the Monte Carlo simulation and select which of the following best describes their distributions:
```{r}
mean(errors)
hist(errors)
```
**C. The errors are symmetrically distributed around 0.**

4. The error Xbar - p is a random variable. In practice, the error is not observed because we do not know  p. Here we observe it because we constructed the simulation. What is the average size of the error if we define the size by taking the absolute value  
|Xbar-p∣?
```{r}
mean(abs(errors))
```

5.The standard error is related to the typical **size** of the error we make when predicting. We say **size** because we just saw that the errors are centered around 0, so thus the average error value is 0. For mathematical reasons related to the Central Limit Theorem, we actually use the standard deviation of `errors` rather than the average of the absolute values to quantify the typical size. What is this standard deviation of the errors?
```{r}
sd(errors)
```

6.  The theory we just learned tells us what this standard deviation is going to be because it is the standard error of Xbar. What does theory tell us is the standard error of Xbar for a sample size of 100?
```{r}
sqrt(p*(1-p)/N)
```

7. 7. In practice, we don’t know p, so we construct an estimate of the theoretical prediction based by plugging in Xbar for p. Compute this estimate. Set the seed at 1 with `set.seed(1)`.
```{r}
set.seed(1)
s <- take_sample(N) %>% mean()
sqrt(s*(1-s)/N)
```
 
8. Note how close the standard error estimates obtained from the Monte Carlo simulation (exercise 5), the theoretical prediction (exercise 6), and the estimate of the theoretical prediction (exercise 7) are. The theory is working and it gives us a practical approach to knowing the typical error we will make if we predict p with Xbar. 
Another advantage that the theoretical result provides is that it gives an idea of how large a sample size is required to obtain the precision we need. Earlier we learned that the largest standard errors occur for  
p=0.5. Create a plot of the largest standard error for N ranging from 100 to 5,000. Based on this plot, how large does the sample size have to be to have a standard error of about 1%?
```{r}
N <- seq(100,5000)
se <- function(N){
  sqrt((0.5)^2/N)
}
result <- sapply(N, se)
plot(N, result)
```
**d. 4000**

9. For sample size N=100, the central limit theorem tells us that the distribution of Xbar is:
**b. approximately normal with expected value p and standard error √p(1−p)/N.**

10. Based on the answer from exercise 8, the error Xbar - p is:
**b. approximately normal with expected value 0 and standard error √p(1−p)/N.**

11. To corroborate your answer to exercise 9, make a qq-plot of the `errors` you generated in exercise 2 to see if they follow a normal distribution.
```{r}
p <- 0.45
N <- 100
errors <- replicate(10000, mean(take_sample(N)-p))
params <- c(mean(errors), sd(errors))

as.data.frame(errors) %>% ggplot(aes(sample = errors)) + geom_qq(dparams = params)
```

12. If p = 0.45 and N = 100 as in exercise 2, use the CLT to extimate the probability that Xbar > 0.5. You can assume you know p = 0.45 for this calculation.
```{r}
m <- mean(errors)
sd <- sd(errors)

#Z > (0.5-m)/s
1-pnorm((0.5-m)/s)
```

13. ASsume you are in a practical situation and you don't know p. Take a sample of size N =100 and obtain a sample average of Xbar = 0.51. What is the CLT approximation for the probability that your error is equal to or larger than 0.01?
```{r}
#-0.01 < Xbar - p < 0.01
#-0.01/SE < Z < 0.01/SE
Xbar <- 0.51
N <- 100
SE = sqrt(Xbar*(1-Xbar)/N)

pnorm(0.01/SE) - pnorm(-0.01/SE)
```


## 15.6 Confidence intervals
_Confidence intervals_ are a very useful concept widely employed by data analysts. A version of these that are commonly seen come from the `ggplot` geometry `geom_smooth`. 

우리는 interval의 넓이와 정확도 두 요인의 일치점을 찾고싶다. -> margin of error

We want to knwo the probability that th einterval [Xbar - 2SEhat(Xbar), Xbar + 2SEhat(Xbar)] contains the true proportion _p_.
To illustrate this, run the Monte Carlo simulation above twice. WE use the same parameters as above:
```{r}
p <- 0.45
N <- 1000
```
And notice that the interval here:
```{r}
x <- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))
x_hat <- mean(x)
se_hat <- sqrt(x_hat * (1 - x_hat) / N)
c(x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat)
```
Keep sampling and creating intervals and you will see the random variation.

To determine the probability that the interval includes p, we need to compute this:
**Pr(Xbar - 1.96SEhat(Xbar) <= p <= Xbar + 1.96SEhab(XBar))**

By subtractin gand dividing the same quantities in all parts of the equation, we get that the above is equivalent to:
**Pr(-1.96 <= (Xbar - p)/SEhat(X) <= 1.96)**

The term in the middle is an approximately normal random variable with expected value 0 and standard error 1, which we have been denoting with  
Z, so we have:
**Pr(-1.96 <= Z <= 1.96)**
which we can quickly compute using:
```{r}
pnorm(1.96) - pnorm(-1.96)
```
proving that we have a 95% probability.


For 99% probability:
Pr(-z <= Z <= z) = 0.99
```{r}
z <- qnorm(0.995)
z
```

We can use this approach for any proportion p: 
we set `z = qnorm(1 - (1 - p)/2)` because 1-(1-p)/2 + (1-p/2) = p

So, for example, for p = 0.95, 1- (1-p)/2 = 0.975 and we get the 1.96 we have been using:
```{r}
qnorm(0.975)
```


### 15.6.1 A Monte Carlo simulation
We can run a Monte Carlo simulation to confirm that, in fact, a 95% confidence interval includes p 95% of the time.
```{r}
N <- 1000
B <- 10000
inside <- replicate(B, {
  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  x_hat <- mean(x)
  se_hat <- sqrt(x_hat * (1 - x_hat) / N)
  between(p, x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat)
})
mean(inside)
```


### 15.6.2 The correct language
When using the theory we described above, it is important to remember that it is the intervals that are random, not p.
interval이 p를 포함할 가능성이지 p가 interval안에 포함되는 것은 기냐 아니냐의 문제지 가능성의 문제가 아니다.


## 15.7 Exercises
```{r}
install.packages("dslabs")
library(dslabs)
data("polls_us_election_2016")

install.packages("tidyverse")
library(tidyverse)
polls <- polls_us_election_2016 %>% filter(enddate >= "2016-10-31" & state == "U.S.")
```


1. For the first poll, you can obtain the samples size and estimated Clinton percentage with:
```{r}
N <- polls$samplesize[1]
x_hat <- polls$rawpoll_clinton[1]/100
```
Assume there are only two candidates and construct a 95% confidence interval for the election night proportion p.
```{r}
z <- 1.96*sqrt(x_hat*(1-x_hat)/N)
p <- c(x_hat-z, x_hat+z)
print(p)
```

2. Now use `dplyr` to add a confidence interval as two columns, call them `lower` and `upper`, to the object `poll`. Then use `select` to show the `pollster`, `enddate`, `x_hat`,`lower`, `upper` variables. Hint: define temporary columns `x_hat` and `se_hat`.
```{r}
x_hat <- polls$rawpoll_clinton/100
N <- polls$samplesize
se_hat <- sqrt(x_hat*(1-x_hat)/N)

poll <- data.frame(lower = x_hat-1.96*se_hat, upper = x_hat+1.96*se_hat, pollster = select(polls, pollster), enddate = select(polls, enddate), x_hat = x_hat)
```

3. The final tally for the popular vote was Clinton 48.2% and Trump 46.1%. Add a column, call it `hit`, to the previous table stating if the confidence interval included the true proportion p = 0.482 or not.
```{r}
poll2 <- ifelse(poll$upper >= 0.482 & poll$lower <= 0.482, T, F) %>% mutate(poll, hit = .)
```

4. For the table you just created, what proportion of confidence intervals included p?
```{r}
poll2 %>% filter(hit == 'TRUE') %>% nrow(.)/nrow(poll2)
```

5. If these confidence intervals are constructed correctly, and the theory holds up, what proportion should include p?
**95%**

6. A much smaller proportion of the polls than expected produce confidence intervals containing  
p
 . If you look closely at the table, you will see that most polls that fail to include p are underestimating. The reason for this is undecided voters, individuals polled that do not yet know who they will vote for or do not want to say. Because, historically, undecideds divide evenly between the two main candidates on election day, it is more informative to estimate the spread or the difference between the proportion of two candidates d, which in this election was 0.482 - 0.461 = 0.021. Assume that there are only two parties and that d = 2p-1, redefine polls as below and re-do exercise 1, but for the difference.
```{r}
polls <- polls_us_election_2016 %>% 
  filter(enddate >= "2016-10-31" & state == "U.S.")  %>%
  mutate(d_hat = rawpoll_clinton / 100 - rawpoll_trump / 100)
```

7. Now repeat exercise 3, but for the difference.
```{r}
d= 0.021

se_d <- se_hat*2

polls_d <- polls %>% mutate(upper_d = d_hat+1.96*se_d, lower_d = d_hat-1.96*se_d)

polls_d <- ifelse(polls_d$upper_d >= d & polls_d$lower_d <= d, T, F) %>% mutate(polls, hit = .)
```


8. Now repeat exercise 4, but for the difference.
```{r}
polls_d %>% filter(hit == 'TRUE') %>% nrow(.)/nrow(polls_d)
```

9. Although the proportion of confidence intervals goes up substantially, it is still lower than 0.95. In the next chapter, we learn the reason for this. To motivate this, make a plot of the error, the difference between each poll’s estimate and the actual d=0.021. Stratify by pollster
```{r}
polls_d <- polls_d %>% mutate(error = abs(d_hat - d))

polls_d %>% ggplot(aes(pollster, error)) + geom_point()
```

10. Redo the plot that you made for exercise 9, but only for pollsters that took five or more polls.
```{r}
selected <- polls_d %>% count(pollster) %>% filter(n >= 5) %>% select(pollster)


polls_d %>% filter(pollster %in% selected$pollster) %>% ggplot(aes(pollster, error)) + geom_point()
```


## 15.8 Power
spread 의 confidence interval은 0을 포함하지 않는 것이 이상적이다.(차이를 말하는 것이므로..)
_power_ is the probability of detecting spreads different from 0.
power는 n의 크기(sample의 크기)와 비례한다.


## 15.9 p values
p-value는 귀무가설이 맞다는 전제 하에, 관측된 통계값 혹은 그 값보다 큰 값이 나올 확률이다.
pvalue 가 매우 낮다면 귀무가설이 틀렸다고 가정한다.
If a 95% confidence interval of the spread does not include 0, we know that the p-value must be smaller than 0.05.


## 15.10 Association tests
```{r}
library(tidyverse)
library(dslabs)
data("research_funding_rates")
research_funding_rates %>% select(discipline, applications_total,success_rates_total) %>% head()
```
We have these values for each gender:
```{r}
names(research_funding_rates)
```
We can compute the totals that were successful and the totals that were not as follows:
```{r}
totals <- research_funding_rates %>% 
  select(-discipline) %>% 
  summarize_all(sum) %>%
  summarize(yes_men = awards_men, 
            no_men = applications_men - awards_men, 
            yes_women = awards_women, 
            no_women = applications_women - awards_women) 
```
So we see that a larger percent of men than women received awards:
```{r}
totals %>% summarize(percent_men = yes_men/(yes_men+no_men),
                     percent_women = yes_women/(yes_women+no_women))
```
But could this be due just to random variability? Here we learn how to perform inference for this type of data.


### 15.10.1 Lady Testing Tea
if the tester is actually guessing, what are the chances that she gets 3 or more correct? Just as we have done before, we can compute a probability under the null hypothesis that she is guessing 4 of each. 


### 15.10.2 Two-by-two tables
```{r}
tab <- matrix(c(3,1,1,3),2,2)
rownames(tab)<-c("Poured Before","Poured After")
colnames(tab)<-c("Guessed before","Guessed after")
tab
```
The function `fisher.test` performs the inference calculations above:
```{r}
fisher.test(tab, alternative="greater")$p.value
```


### 15.10.3 Chi-square Test
Imagine we have 290, 1,345, 177, 1,011 applicants, some are men and some are women and some get funded, whereas others don’t. We saw that the success rates for men and woman were:
```{r}
totals %>% summarize(percent_men = yes_men/(yes_men+no_men),
                     percent_women = yes_women/(yes_women+no_women))
```
respectively. Would we see this again if we randomly assign funding at the overall rate:
```{r}
rate <- totals %>%
  summarize(percent_total = 
              (yes_men + yes_women)/
              (yes_men + no_men +yes_women + no_women)) %>%
  pull(percent_total)
rate
```
The Chi-square test answers this question. The first step is to create the two-by-two data table:
```{r}
two_by_two <- data.frame(awarded = c("no", "yes"), 
                     men = c(totals$no_men, totals$yes_men),
                     women = c(totals$no_women, totals$yes_women))
two_by_two
```
The general idea of the Chi-square test is to compare this two-by-two table to what you expect to see, which would be:
```{r}
data.frame(awarded = c("no", "yes"), 
       men = (totals$no_men + totals$yes_men) * c(1 - rate, rate),
       women = (totals$no_women + totals$yes_women) * c(1 - rate, rate))
```
We can see that more men than expected and fewer women than expected received funding. However, under the null hypothesis these observations are random variables. The Chi-square test tells us how likely it is to see a deviation this large or larger. This test uses an asymptotic result, similar to the CLT, related to the sums of independent binary outcomes. The R function `chisq.test` takes a two-by-two table and returns the results from the test:
```{r}
chisq_test <- two_by_two %>% select(-awarded) %>% chisq.test()
```
We see that the p-value is 0.0509:
```{r}
chisq_test$p.value
```



### 15.10.4 The odds ratio

odds of being funded if men:
```{r}
odds_men <- with(two_by_two, (men[2]/sum(men)) / (men[1]/sum(men)))
odds_men
```

odds of being funded if women:
```{r}
odds_women <- with(two_by_two, (women[2]/sum(women)) / (women[1]/sum(women)))
odds_women
```

The odds ratio is the ratio for these two odds: how many times larger are the odds for men than for women?
```{r}
odds_men/odds_women
```


### 15.10.5 Confidence intervals for the odds ratio
the odds ratio is not only a ratio, but a ratio of ratios. Therefore, there is no simple way of using, for example, the CLT.

However, statistical theory tells us that when all four entries of the two-by-two table are large enough, then the log of the odds ratio is approximately normal with standard error

√1/a+1/b+1/c+1/d

This implies that a 95% confidence interval for the log odds ratio can be formed by:

log(ad/bc)+-1.96√1/a+1/b+1/c+1/d

```{r}
log_or <- log(odds_men / odds_women)
se <- two_by_two %>% select(-awarded) %>%
  summarize(se = sqrt(sum(1/men) + sum(1/women))) %>%
  pull(se)
ci <- log_or + c(-1,1) * qnorm(0.975) * se
```
If we want to convert it back to the odds ratio scale, we can exponentiate:
```{r}
exp(ci)
```
Note that 1 is not included in the confidence interval which must mean that the p-value is smaller than 0.05. We can confirm this using:
```{r}
2*(1 - pnorm(log_or, 0, se))
```
This is a slightly different p-value than that with the Chi-square test. This is because we are using a different asymptotic approximation to the null distribution. To learn more about inference and asymptotic theory for odds ratio, consult the Generalized Linear Models book by McCullagh and Nelder.


### 15.10.6 Small count correction
log odds ratio는 0이 있으면 안되므로 만약 0이 포함되면 adding 0.5를 each cell에 적용.



### 15.10.7 Large samples, small p-values
Note that the relationship between odds ratio and p-value is not one-to-one. It depends on the sample size. So a very small p-value does not necessarily mean a very large odds ratio. Notice what happens to the p-value if we multiply our two-by-two table by 10, which does not change the odds ratio:
```{r}
two_by_two %>% select(-awarded) %>%
  mutate(men = men*10, women = women*10) %>%
  chisq.test() %>% .$p.value
```




## 15.10 Exercise
1. A famous athlete has an impressive career, winning 70% of her 500 career matches. However, this athlete gets criticized because in important events, such as the Olympics, she has a losing record of 8 wins and 9 losses. Perform a Chi-square test to determine if this losing record can be simply due to chance as opposed to not performing well under pressure.
```{r}

```


2. Why did we use the Chi-square test instead of Fisher’s exact test in the previous exercise?